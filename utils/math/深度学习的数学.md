## 深度学习的数学

### 一、神经网络的思想

#### 1. 生物学上的神经元

谈到神经网络的想法，需要从生物学上的神经元（neuron）开始说起。从生物学的扎实的研究成果中，我们可以得到以下关于构成大脑的神经元的知识。
(i) 神经元形成网络。
(ii) 对于从其他多个神经元传递过来的信号，如果它们的和不超过某个固定大小的值（阈值），则神经元不做出任何反应。
(iii) 对于从其他多个神经元传递过来的信号，如果它们的和超过某个固定大小的值（阈值），则神经元做出反应（称为点火），向另外的神经元传递固定强度的信号。
(iv) 在(ii) 和(iii) 中，从多个神经元传递过来的信号之和中，每个信号对应的权重不一样。

![image-20201103143606937](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201103143606937.png)

将神经元的工作在数学上抽象化，并以其为单位人工地形成网络，这样的人工网络就是神经网络。将构成大脑的神经元的集合体抽象为数学模型，这就是神经网络的出发点。

#### 2. 神经元工作的数学表示

- 神经元点火的结构：
  (i) 来自其他多个神经元的信号之和成为神经元的输入。
  (ii) 如果这个信号之和超过神经元固有的阈值，则点火。
  (iii) 神经元的输出信号可以用数字信号0 和1 来表示。即使有多个输出端，其值也是同一个。

- 数学方式表示点火的判定条件。

  ![image-20201103143620107](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201103143620107.png)

  来自相邻神经元1、2、3 的输入信号分别为 $x_1,x_2,x_3$ ，则神经元的输入信号之和可以用 $w_1x_1 + w_2x_2 + w_3x_3$ 表示。式中的 $w_1,w_2,w_3$ 是输入信号 $x_1,x_2,x_3$ 对应的权重（weight）。根据(ii)，神经元在信号之和超过阈值时点火，不超过阈值时不点火。于是，利用式(1)，点火条件可以如下表示。这里，是该神经元固有的阈值。

$$
无输出信号（y=0）：w_1x1+w_2x_2+w_3x_3 < \theta
$$

$$
有输出信号（y=1）：w_1x_1+w_2x_2+w_3x_3 \geq \theta
$$

- 点火条件的图形化

  ![image-20201103143630963](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201103143630963.png)

![image-20201103143636116](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201103143636116.png)
$$
点火的式子：y = u(w_1x_1+w_2x_2+w_3x_3-\theta)
$$

#### 3. 激活函数

- 神经单元（简化、抽象化的）

  ![image-20201103143645931](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201103143645931.png)

- 激活函数

  我们将含单位阶跃函数的点火式子 $y = u(w_1x_1+w_2x_2+w_3x_3-\theta)$ ，一般化一下如下所示。
  $$
  y = a(w_1x_1+w_2x_2+w_3x_3-\theta)
  $$
  这里的函数是建模者定义的函数，称为激活函数（activation function）。$x_1,x_2,x_3$ 是模型允许的任意数值，是函数能取到的任意数值。这个式子就是今后所讲的神经网络的出发点。

  ![image-20201103143700113](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201103143700113.png)

- Sigmoid 函数

  激活函数的代表性例子是Sigmoid 函数，其定义如下所示。
  $$
  \sigma(z) = \frac 1 {1+e^{-z}}(e = 2.718281...)
  $$
  Sigmoid 函数的输出值是大于0 小于1 的任意值。此外，该函数连续、光滑，也就是说可导。这两种性质使得Sigmoid 函数很容易处理。

- Tanh 函数
  $$
  f(z) = tanh(z) = \dfrac{e^z-e^{-z}}{e^z+e^{-z}}
  $$
  
- ReLU 函数
  $$
  \phi(x) = max(0,x)
  $$
  
- softmax 函数

- 偏置

式 $y = a(w_1x_1+w_2x_2+w_3x_3-\theta)$ 中只有 $\theta$ 带有负号，这看起来不漂亮。数学不喜欢不漂亮的东西。另外，负号具有容易导致计算错误的缺点，因此，我们将 $-\theta$ 替换为 $b$ 。经过这样处理，式子变漂亮了，也不容易发生计算错误。这个称为偏置（bias）。
$$
y = a(w_1x_1+w_2x_2+w_3x_3+b)
$$
![image-20201103143732253](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201103143732253.png)

#### 4. 神经网络

![image-20201103143739125](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201103143739125.png)

- 输入层负责读取给予神经网络的信息。属于这个层的神经单元没有输入箭头，它们是简单的神经单元，只是将从数据得到的值原样输出。

- 隐藏层和输出层的神经单元执行前面的处理操作。在神经网络中，这是实际处理信息的部分。隐藏层具有提取输入图像的特征的作用。

- 这个简单的神经网络的特征是，前一层的神经单元与下一层的所有神经单元都有箭头连接，这样的层构造称为全连接层（fully connected layer）。这种形状对于计算机的计算而言是十分容易的。

#### 5. 重要的隐藏层

- 隐藏层肩负着特征提取（feature extraction）的重要职责。
- 建立一个神经网络，用来识别通过4×3 像素的图像读取的手写数字0 和1。学习数据是64 张图像，其中像素是单色二值。书中用恶魔兴奋来理解。

#### 6. 网络自学习的神经网络

神经网络中比较重要的一点就是利用网络自学习算法来确定权重大小。神经网络是怎样学习的呢？思路极其简单：计算神经网络得出的预测值与正解的误差，确定使得误差总和达到最小的权重和偏置。这在数学上称为模型的最优化（下图）。

![image-20201103143750044](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201103143750044.png)

关于预测值与正解的误差总和，有各种各样的定义。本书采用的是最古典的定义：针对全部学习数据，计算预测值与正解的误差的平方（称为平方误差），然后再相加。这个误差的总和称为代价函数（cost function），用符号表示（T 是Total 的首字母）。



### 二、神经网络的数学基础

#### 1.神经网络所需的函数

1. 一次函数：$y = ax+b$ ，神经单元的加权输入可以表示为一次函数关系。

2. 二次函数：$y = ax^2+bx+c $ ，代价函数可以用二次函数关系表示。

3. 单位阶跃函数：$$u(x) = \left\{ \begin{matrix}0 &(x<0)\\ 1&(x\geq 0) \end{matrix}\right.$$ ，神经网络的原型模型是用单位阶跃函数作为激活函数的。

   ![image-20201025181125986](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201025181125986.png)

4. **Sigmoid** 函数： $$\sigma(x) = \dfrac{1}{1+e^{-x}} = \dfrac1{1+exp(-x)}$$  ，**Sigmoid** 函数是神经网络中具有代表性的激活函数。

   ![image-20201025181146265](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201025181146265.png)

5. 正态分布的概率密度函数：$f(x) = \dfrac1{\sqrt{2\pi}\sigma}e^{-\frac{(x-u)^2}{2\sigma^2}}$ ，在神经网络的计算中经常用到正态分布随机数作为初始值。

   ![image-20201025182441869](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201025182441869.png)

#### 2. 有助于理解神经网络的数列和递推关系式

1. 数列的含义（神经网络的世界中出现的数列是有限项的数列，这样的数列称为有穷数列。）
2. 数列的通项公式（神经单元的加权输入以及输出可以看成数列，如 $a_j^l$ 表示第 $l$ 层的第 $j$ 个神经单元的输出值。）
3. 数列与递归关系式（后述的误差反向传播法就是通过计算机所擅长的递推关系式这一计算方法来进行神经网络的计算的。）



#### 3. 神经网络中经常看到的 $\sum$ 符号

1. $\sum$ 符号的含义：$\sum$ 符号可以简洁地表示数列的总和

2. 对于数列 $\{a_n\}$，$\sum$ 符号的定义式为：$\sum\limits_{k=1}^n = a_1 + a_2 + \dots + a_n$ 

3. $\sum$ 符号具有线性性质。这是与微积分共通的性质，可以在式子变形中使用。
   $$
   \sum\limits_{k=1}^n(a_k+b_k) = \sum\limits_{k=1}^n{a_k} + \sum\limits_{k=1}^{n}{b_k}, \sum\limits_{k=1}^nca_k = c\sum\limits_{k=1}^na_k
   $$

#### 4. 有助于理解神经网络的向量基础

1. 向量的表示方法

   ![image-20201025190528272](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201025190528272.png)

2. 向量的坐标表示

   ![image-20201025190640475](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201025190640475.png)

3. 向量的大小，从直观上来讲，表示向量的箭头的长度称为这个向量的大小。向量 $a$ 的大小 $|a|$ 用 表示。
   $$
   若 \overrightarrow a = （a_1,a_2,a_3),则\ \ |\overrightarrow a| =\sqrt{a_1^2+a_2^2+a_3^2}
   $$

4. 向量的内积，即两个向量 $\overrightarrow a,\overrightarrow b$ 的内积 $\overrightarrow a \cdot \overrightarrow b$ 的定义如下： 
   $$
   {\overrightarrow a\cdot \overrightarrow b } = |\overrightarrow a||\overrightarrow b|cos\theta (\theta 为\overrightarrow a,\overrightarrow b的夹角)
   $$
   ![image-20201025193825610](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201025193825610.png)

5. 柯西 - 施瓦茨不等式

   - 定义
     $$
      -|\overrightarrow a||\overrightarrow b| \leq |\overrightarrow a||\overrightarrow b|cos\theta \leq |\overrightarrow a||\overrightarrow b|
     $$
     ![image-20201025194247364](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201025194247364.png)

   - 根据柯西 - 施瓦茨不等式可得一下三点事实：

     - a. 当两个向量方向相反时，内积取得最小值。(**后续梯度下降法基本原理**)
     - b. 当两个向量不平行时，内积取平行时的中间值。
     - c. 当两个向量方向相同时，内积取得最大值。

6. 内积的坐标表示
   $$
   当\ \ \overrightarrow a = (a_1,a_2,a_3), \overrightarrow b = (b_1,b_2,b_3)\ \ 时， \overrightarrow a\cdot \overrightarrow b = a_1b_1+a_2b_2+a_3b_3
   $$

7. 向量的一般化，将二维以及三维空间中的向量公式推广到任意的 n 维空间

   - 向量的坐标表示：$\overrightarrow a = (a_1,a_2,\dots,a_n)$ 

   - 内积的坐标表示：对于两个向量 $\overrightarrow a = (a_1,a_2,\dots,a_n) \ , \ \overrightarrow b = (b_1,b_2,\dots,b_n)$ ，其内积公式如下：
     $$
     \overrightarrow a \cdot \overrightarrow b = a_1b_1 + a_2b_2 + \dots + a_nb_n 
     $$

   - 柯西 - 施瓦茨不等式：$$ -|\overrightarrow a||\overrightarrow b| \leq |\overrightarrow a||\overrightarrow b|cos\theta \leq |\overrightarrow a||\overrightarrow b|$$ 

   - 神经单元的输入 $\overrightarrow x = (x_1,x_2,\cdots,x_n)$ ，输入的权重 $\overrightarrow w = (w_1,w_2,\cdots,w_n)$ 。那么加权输入可以表示为内积形式：
     $$
     input= \overrightarrow w \cdot \overrightarrow x + b
     $$
     ![image-20201025210539645](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201025210539645.png)

     

#### 5. 有助于理解神经网络的矩阵基础

1. 什么是矩阵? $A_{mn}$

   ![image-20201025210745791](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201025210745791.png)

   

2. 矩阵相等（对应元素相等）

3. 矩阵的和、差、常数倍（对应元素和、差，所有元素常数倍）

4. 矩阵的乘积

   ![image-20201025211200105](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201025211200105.png) 

5. **Hadamard** 乘积

   对于相同形状的矩阵$A,B$ ，将相同位置的元素相乘，由此产生的矩阵称为矩阵 $A,B$ 的 **Hadamard** 乘积，用 $A\bigodot B$ 表示。

6. 转置矩阵 

   将矩阵 $A$ 的第 $i$ 行第 $j$ 列的元素与第 $j$ 行第 $i$ 列的元素交换，由此产生的矩阵称为矩阵 $A$ 的转置矩阵（transposed matrix），用 $A^T$ 表示。

#### 6. 神经网络的导数基础

1. 导数的定义 
   $$
   f^\prime (x) = \lim_{\Delta x \to 0} \dfrac{f(x+\Delta x) - f(x) }{\Delta x}
   $$

2. 神经网络中用到的函数的导数公式
   $$
   (e)^{\prime} = 0,(x)^\prime = 1,(x^2)^\prime = 2x,(e^x)^\prime = e^x ,(e^{-x})^\prime = -e^{-x}
   $$

3. 导数符号
   $$
   f^\prime(x) = \dfrac{dy}{dx}
   $$

4. 导数的性质

   导数具有线性性，即和的导数为导数的和，常数倍的导数为导数的常数倍。(导数的线性性是后述的误差反向传播法背后的主角。)
   $$
   \{f(x) \pm g(x)\}^\prime = f^\prime(x) \pm g^\prime(x),\{cf(x)\}^\prime = cf^\prime(x)
   $$

5. **Sigmoid** 函数的导数

   - 分数函数的导数
     $$
     \{\dfrac1{f(x)}\}^\prime = -\dfrac{f^\prime(x)}{\{f(x)\}^2}
     $$

   - **Sigmoid** 函数 $\sigma(x) = \dfrac1{1+e^{-x}}$ 的导数
     $$
     \sigma\prime(x) = \sigma(x)(1-\sigma(x))
     $$
     在后续梯度下降中利用这个公式求导会十分方便。

6. 最小值的条件

   $f^\prime(a) = 0$ 是函数 $f(x)$ 在 $x=a$ 处取得最小值的必要条件。从下面的函数 $y=f(x)$ 的图像可以清楚地看出这一点。

   ![image-20201026140601121](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201026140601121.png)

   在通过后述的梯度下降法求最小值时，这个性质有时会成为很大的障碍。

#### 7. 神经网络的偏导数基础

1. 多变量函数：$f(x,y) = x^2+y^2$

2. 偏导数
   $$
   \dfrac {\partial z}{\partial x} = \dfrac{\partial f(x,y)}{\partial x} = \lim_{\Delta x \to 0}\dfrac{f(x+\Delta x,y)-f(x,y)}{\Delta x}
   $$

   $$
   \dfrac {\partial z}{\partial y} = \dfrac{\partial f(x,y)}{\partial y} = \lim_{\Delta y \to 0}\dfrac{f(x,y+\Delta y)-f(x,y)}{\Delta y}
   $$

3. 多变量函数的最小值条件

   - 函数 $z=f(x,y)$ 取得最小值的必要条件是 ，$\dfrac{\partial f}{\partial x}=0,\dfrac{\partial f}{\partial y}=0$。

   - 拉格朗日乘数法

     在实际的最小值问题中，有时会对变量附加约束条件，例如下面这个问题。

     例：当$x^2+y^2 = 1$ 时，求 $x+y$ 的最小值。

     这种情况下我们使用拉格朗日乘数法。这个方法首先引入参数 $\lambda$ ，创建下面的函数 $L$ 。
     $$
     L = f(x,y)-\lambda g(x,y) = (x+y) - \lambda(x^2 + y^2-1)
     $$
     利用最小值的必要条件可求得 $x,y,\lambda$ ：
     $$
     \dfrac {\partial L}{\partial x} = 1-2\lambda x = 0,\dfrac{\partial L}{\partial y} = 1-2\lambda y = 0
     $$
     在用于求性能良好的神经网络的正则化技术中，经常会使用该方法。



#### 8. 误差反向传播法必需的链式法则

1. 神经网络和复合函数
   $$
   \begin{cases} z = f(x_1,x_2,\cdots,c_n) = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b\\
   y = a(z) = \dfrac{1}{1+e^{-z}}\end{cases}
   $$

2. 单变量函数的链式法则
   $$
   \begin{cases}y = f(u) \\ u = g(x) \end{cases} \Rightarrow \dfrac{dy}{dx} =\dfrac{dy}{du}\dfrac{du}{dx}
   $$

3. 多变量函数的链式法则          
   $$
   \begin{cases}z = z(u,v) \\ u = u(x,y) \\v = v(x,y) \end{cases} \Rightarrow \begin{cases}\dfrac{\partial z}{\partial x} = \dfrac{\part z}{\part u}\dfrac{\part u}{\part x} + \dfrac{\part z}{\part v}\dfrac{\part v}{\part x} \\ \dfrac{\part z}{\part y} = \dfrac{\part z}{\part u}\dfrac{\part u}{\part y} + \dfrac{\part z}{\part v}\dfrac{\part v}{\part y}\end{cases}
   $$

#### 9. 梯度下降的基础：多变量函数的近似公式

1. 单变量函数的近似公式
   $$
   f^\prime (x) = \lim_{\Delta x \to 0} \dfrac{f(x+\Delta x) - f(x) }{\Delta x} \Rightarrow f(x + \Delta x) \approx f(x) + f^\prime(x) \Delta x
   $$

2. 多变量函数的近似公式
   $$
   f(x+\Delta x,y+\Delta y) \approx f(x,y) + \dfrac{\part f(x,y)}{\part x}\Delta x + \dfrac{\part f(x,y)}{\part y}\Delta y
   $$
   定义 $\Delta z = f(x+\Delta x,y+ \Delta y) -f(x,y)$ ，简化上述公式如下：
   $$
   \Delta z \approx \dfrac{\part z}{\part x}\Delta x + \dfrac{\part z}{\part y}\Delta y
   $$

3. 近似公式的向量表示

   推广一下，z 为三个变量 w、x、y 的函数时，近似公式如下所示：
   $$
   \Delta z \approx \dfrac{\part z}{\part w}\Delta w + \dfrac{\part z}{\part x}\Delta x + \dfrac{\part z}{\part y}\Delta y
   $$
   三个变量的函数的近似公式可以表示为如下两个向量内积 $\nabla z \cdot \Delta x$ 的形式。
   $$
   \nabla z = (\dfrac{\part z}{\part w},\dfrac{\part z}{\part x},\dfrac{\part z}{\part y}),\qquad \Delta x = (\Delta w, \Delta x, \Delta y)
   $$

#### 10. 梯度下降法的含义与公式

1. 梯度下降法的思路

   - 问题：已知函数 $z=f(x,y)$ ，如何求使函数取得最小值的 x，y 呢？
   - 解法一：解关系方程 $\begin{cases}\dfrac{\part f(x,y)}{\part x} = 0 \\ \dfrac{\part f(x,y)}{\part y} = 0\end{cases}$ ，不容易求解
   - 梯度下降思路：慢慢地移动图像上的点进行摸索，从而找出函数的最小值。

2. 近似公式和内积的关系

   函数 $z=f(x,y)$ ，由近似公式，以下关系成立：
   $$
   \Delta z = \dfrac{\part z}{\part x}\Delta x + \dfrac{\part z}{\part y}\Delta y  \qquad (2)
   $$
   两个向量：
   $$
   (\dfrac{\part f(x,y)}{\part x},\dfrac{\part f(x,y)}{\part y},\quad (\Delta x,\Delta y)) \qquad (3)
   $$
   ![image-20201101104452631](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201101104452631.png)

3. 向量内积的回顾

   我们来考察两个固定大小的非零向量 $\overrightarrow a ,\overrightarrow b$ 。当 $\overrightarrow a$ 的方向与 $\overrightarrow b$ 相反时，即 $\overrightarrow b + k \overrightarrow a = 0 (k>0) \quad (4)$ 时，内积 $\overrightarrow a \cdot \overrightarrow b$ 取最小值。这是梯度下降法的数学基础。

   ![image-20201101104957552](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201101104957552.png)

   

   

4. 二变量函数的梯度下降法的基本式

   当 x 改变 $\Delta x$，y 改变 $\Delta y$ 时，函数 f(x,y) 的变化 $\Delta z$ 为式 (2)，可以表示为式 (3) 的两个向量的内积。根据式 (4)，当两个向量方向相反时，内积取最小值。也就是说，当式 (3) 的两个向量的方向恰好相反时，式 (2)的 达到最小（即减小得最快）。根据以上讨论我们可以知道，从点 (x，y) 向点 $(x+\Delta x,y+\Delta y)$ 移动时，当满足以下关系式时，函数 $z=f(x,y)$ 减小得最快。这个关系式就是二变量函数的梯度下降法的基本式。
   $$
   (\Delta x,\Delta y) = -\eta (\dfrac{\part f(x,y)}{\part x},\dfrac{\part f(x,y)}{\part y}) (\eta > 0) \qquad (5)
   $$
   式（5）右边的向量 $(\dfrac{\part f(x,y)}{\part x},\dfrac{\part f(x,y)}{\part y})$ 称为函数 $f(x,y)$ 在点 $(x,y)$ 处的梯度（gradient）。这个名称来自于它给出了最陡的坡度方向。（x，y)的移动公式如下：
   $$
   (x,y) = (x + \Delta x,y+\Delta y)  = (x - \eta\dfrac{\part f(x,y)}{\part x},y-\eta\dfrac{\part f(x,y)}{\part y}) \qquad (6)
   $$

5. 梯度下降法及其用法

   要寻找函数的最小值，可以利用式 (5) 找出减小得最快的方向，沿着这个方向依照上述 (6) 稍微移动。在移动后到达的点处，再次利用式 (5) 算出方向，再依照上述 (6) 稍微移动。通过反复进行这样的计算，就可以找到最小值点。这种寻找函数 f(x,y) 的最小值点的方法称为二变量函数的梯度下降法。

   ![image-20201101141229575](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201101141229575.png)

6. 将梯度下降法推广到三个变量以上的情况

   - 多元变量函数 $f$ 减小得最快时满足以下关系式：
     $$
     (\Delta x_1,\Delta x_2,\cdots ,\Delta x_n) = -\eta(\dfrac{\part f}{\part x_1},\dfrac{\part f}{\part x_2},\cdots,\dfrac{\part f}{\part x_n}) \qquad (7)
     $$

   - 函数 $f$ 在点 $(x_1,x_2,\cdots,x_n)$ 处的梯度：
     $$
     (\dfrac{\part f}{\part x_1},\dfrac{\part f}{\part x_2},\cdots,\dfrac{\part f}{\part x_n})
     $$

7. 哈密顿算子 $\nabla$

   - 定义：
     $$
     \nabla f = (\dfrac{\part f}{\part x_1},\dfrac{\part f}{\part x_2},\cdots,\dfrac{\part f}{\part x_n})
     $$

   - 式（7）的简洁表示
     $$
     (\Delta x_1,\Delta x_2,\cdots ,\Delta x_n) = -\eta \nabla f(\eta > 0)
     $$

8. $\eta$ 的含义以及梯度下降的要点

   到目前为止， 只是简单地表示正的微小常数。而在实际使用计算机进行计算时，如何恰当地确定这个 是一个大问题。从式 (5) 的推导过程可知， 可以看作人移动时的“步长”，根据 的值，可以确定下一步移动到哪个点。如果步长较大，那么可能会到达最小值点，也可能会直接跨过了最小值点（左图）。而如果步长较小，则可能会滞留在极小值点（右图）。

   ![image-20201101142718180](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201101142718180.png)

   在神经网络的世界中， 称为**学习率**。遗憾的是，它的确定方法没有明确的标准，只能通过反复试验来寻找恰当的值。

#### 11. 用 Excel 体验梯度下降

#### 12. 最优化问题和回归分析

在为了分析数据而建立数学模型时，通常模型是由参数确定的。在数学世界中，最优化问题就是如何确定这些参数从数学上来说，确定神经网络的参数是一个最优化问题，具体就是对神经网络的参数（即权重和偏置）进行拟合，使得神经网络的输出与实际数据相吻合。

1. 回归分析

   由多个变量组成的数据中，着眼于其中一个特定的变量，用其余的变量来解释这个特定的变量，这样的方法称为回归分析。

2. 代价函数（拓展）

3. 模型参数的个数

   模型的参数个数大于数据规模时又如何呢？当然，这时参数就不确定了。因此，要确定模型，就必须准备好规模大于参数个数的数据。

4. 常量和变量

   根据不同的角度，常数、变量是变幻不定的。从数据的角度来看，回归方程的 x、y 为变量，从代价函数的角度来看，p 、q 为变量。

### 三、神经网络的最优化

#### 3.1 神经网络的参数和变量

1. 参数和变量

   ![image-20201108100038171](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201108100038171.png)

   - 权重 $w_1,w_2,w_3$ 与偏置 b 为**参数**，输入 $x_1,x_2,x_3$ 、加权输入 $z_1$ 、神经单元的输出 $a_1$ 为**变量**，变量的值根据学习数据的学习实例而变化。

   

2. 神经网络中用到的参数和变量数量庞大

   - 在实际进行神经网络的计算时，往往会被数量庞大的参数和变量所困扰。构成神经网络的神经单元的数量非常大，相应地表示偏置、权重、输入、输出的变量的数目也变得非常庞大。因此，参数和变量的表示需要统一标准。

3. 神经网络中用到的变量名和参数名    

   ![a](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\a.png)

   - 注意神经网络中的变量和参数的表示方法：$x_i,w^l_{ji},z^l_j,b^l_j,a^l_j$

4. 输入层相关的变量

   - 输入层为神经网络的数据入口，如果表示输入层的输入的变量名依次为 $x_1,x_2,\cdots$，由于输入层中神经单元的输入和输出为同一值，那么它们也是输出的变量名。

5. 变量值的表示方法

   - 在前面讲解变量名的表格中，$x_i,z^l_j,a^l_j$ 为变量，它们的值根据学习数据的学习实例而变化。通过例题来说明的话，若具体地给出了学习数据的一个图像，则 $x_i,z^l_j,a^l_j$ 就变成了数值，而不是变量。

6. 本书中使用的神经单元符号和变量名

   - 本书中的示意图都是将参数和变量写在一个神经单元的周围，这就导致图看起来非常吃力。因此，之后我们将根据情况使用如下所示的标有参数和变量的神经单元示意图。

     ![image-20201108112136880](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201108112136880.png)

#### 3.2 神经网络的变量的关系式

1. 输入层的关系式

   - 输入层（层 1）神经网络的信息入口。这个层的第 i 个神经单元的输入与输出为同一值 $x_i$ 
     $$
     x_i = a_i^1
     $$
     

2. 隐藏层的关系式

   <img src="C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201108153728169.png" alt="image-20201108153728169" style="zoom:80%;" />
   $$
   \begin{cases}z^2_1 =w_{11}^2x_1 + w_{12}^2x_2 + \cdots + w^2_{1\ 12}x_{12} + b^2_1
   \\ z^2_2 =w_{21}^2x_1 + w_{22}^2x_2 + \cdots + w^2_{2\ 12}x_{12} + b^2_2
   \\ z^2_3 =w_{31}^2x_1 + w_{32}^2x_2 + \cdots + w^2_{3\ 12}x_{12} + b^3_2
   \\a^2_1 = a(z^2_1),a^2_2 = a(z^2_2),a^2_3 = a(z^2_3) \end{cases}
   $$
   
3. 输出层的关系式

   <img src="C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201108155850696.png" alt="image-20201108155850696" style="zoom:80%;" />

   ![image-20201108155905322](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201108155905322.png)

   

#### 3.3 学习数据和正解

1. 回归分析的学习数据和正解

   - 利用事先提供的数据（学习数据）来确定权重和偏置，这在神经网络中称为学习。学习的逻辑非常简单，使得神经网络算出的预测值与学习数据的正解的总体误差达到最小即可。

2. 神经网络的学习数据和正解

   <img src="C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201108160246480.png" alt="image-20201108160246480" style="zoom:80%;" />

   在回归分析的情况下，如上所示，由于全部数据都整合在表格里，所以预测值和正解的关系很容易理解。而在神经网络的情况下，则通常无法将预测值和正解整合在一张表里。

3. 正解的表示

   <img src="C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201108181133455.png" alt="image-20201108181133455" style="zoom:80%;" />

   ![image-20201108181143636](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201108181143636.png)

4. 交叉熵

   - 交叉熵将上述误差函数 (2) 替换为下式。

     ![image-20201108181231812](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201108181231812.png)

   - 上式中， n 为数据的规模。利用这个交叉熵和 Sigmoid 函数，可以消除 Sigmoid 函数的冗长性，提高梯度下降法的计算速度。

#### 3.4 神经网络的代价函数

- 向神经网络提供学习数据，并确定符合学习数据的权重和偏置，这个过程称为学习。这在数学上一般称为最优化，最优化的目标函数是代价函数。本节我们就来看一下代价函数的相关内容。

1. 表示模型准确度的代价函数

   - 用于数据分析的数学模型是由参数确定的。在神经网络中，权重和偏置就是这样的参数。通过调整这些参数，使模型的输出符合实际的数据（在神经网络中就是学习数据），从而确定数学模型，这个过程在数学上称为最优化（2-12 节），在神经网络的世界中则称为学习（1-7节）。
   - 在数学中，用模型参数表示的总体误差的函数称为代价函数，此外也可以称为损失函数、目的函数、误差函数等。

2. 回归分析的回顾

3. 最优化的基础：代价函数的最小化

   - 最优化的思想可以形象地表示为下图。

     ![image-20201108183344121](C:\Users\shengkai\AppData\Roaming\Typora\typora-user-images\image-20201108183344121.png)

     

4. 神经网络的代价函数
   $$
   \begin{align}
     & J(\Theta) = -\frac{1}{m} \left[ \sum\limits_{i=1}^{m} \sum\limits_{k=1}^{k} {y_k}^{(i)} \log {(h_\Theta(x^{(i)}))}_{k} + \left( 1 - y_k^{(i)} \right) \log \left( 1- {\left( h_\Theta \left( x^{(i)} \right) \right)}_{k} \right) \right] 
   \end{align}
   $$
   
5. 参数的个数和数据的规模

   - 我们在 2-12 节考察过，如果数据的规模（即构成数据的元素个数）小于确定数学模型的参数个数的话，就无法确定模型。因此在例 2中，学习用的图像至少需要 47 张（权重和偏置总数）。

6. 神经网络和回归分析的差异

   - (i) 相比回归分析中使用的模型的参数，神经网络中使用的参数的数目十分巨大。
   - (ii) 线性回归分析中使用的函数为一次式，而神经网络中使用的函数（激活函数）不是一次式。因此，在神经网络的情况下，代价函数变得很复杂。

7. 用Excel将代价函数最小化

#### 3.5 用Excel体验神经网络

### 四、神经网络和误差反向传播

#### 4.1 梯度下降法的回顾

#### 4.2 神经单元的误差

#### 4.3 神经网络和误差反向传播

